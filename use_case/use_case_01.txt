This is requirement of document embedding requirements for RAG-CHAT app, it's mainly about the use upload document/web url/blog etc. for embedding into vector database for building knowledage base.

I also call this document embedding process as pre-process of our chatbot since those embedded trunks can be used in the Q/A of the RAG.

Now, let's focus on the pre-process requirement, the pre-process is about embedding documents loaded from index_log from postgres database if the records status is PENDING, detailed cases please find in below:
1.User can add an index_log via api POST:/embedding/docs. The request will go through a checking,if the provided doc(source field in db, here is file path or url) and doc type(e.g. docx,csv) already exists,
 then will check whether the checksum of its content has changed, if no change has been detected, then print log and return 200 wth response - {"message": "the source has already been processed", "source":xxx,"sourceType":xxx};
If detected some change on document, then use the metadata(source+source_type) to retrieval embedded trunks and remove then from vector database, set index log status to be PENDING.
If the doc does not exist, then insert index log to database and set status as PENDING.
2.User can get the process status by index log id
3.DocEmbeddingsProcessor will have a scheduled job to scan the index log table to load the pending index log, load one each time to process, the load index log method is protected by a distributed lock(pg based), if loaded
an index log, update its status to be IN_PROGRESS, and call _load method to load documents to save into vector database.
4.Distributed lock used here is a simple implement, mainly leverage the unique constants in database,the key column of the lock table has id,lock_key,instance_name, the lock_key has unique index.
5.The _load(self, index_log: IndexLog) method should do the followings:
    5.1 Use the source_type to get correct load from loader.loader_factories.DocumentLoaderFactory.get_loader.
    5.2 Use the loader to load documents
    5.3 Add metadata(source,source_type,checksum obtained from index_log) to the loaded trunks.
    5.4 Save into vector database.
6.In all above cases, the auditing fields created_at/created_by/modified_at/modified_by should be updated properly.
7.If any errors are captured during document process, update the error stack info to error_message field in index log table.
8.Expose an endpoint to list index logs by pagination, so that later can be integrated with web ui to display that.
9.Enable search function on web page for index,wildcard search on source,createdBy,modifiedBy.

Others:
1.Table schema of the index log table.
```
drop table if exists index_logs;
drop index if exists unique_index_log;
create table if not exists index_logs (
    id bigserial primary key,
    source varchar(1024) not null,
    source_type varchar(128) not null,
    checksum varchar(255) not null,
    created_at timestamp not null,
    created_by varchar(128) not null,
    modified_at timestamp not null,
    modified_by varchar(128) not null,
    status varchar(128) not null,
    error_message text,
    constraint uix_source_source_type unique (source, source_type),
    constraint uix_checksum unique (checksum),
);

drop index if exists idx_source_checksum;
create index if not exists idx_source_checksum on index_logs (source, source_type, checksum);
```
2.For all API we build, the mandatory headers will be user-id, session-id and request-id, these 3 key information should be used as important API tracing key in logs.


